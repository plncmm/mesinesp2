{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import mesinesp2.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = mesinesp2.tokenizer.get_descriptions(\"../data/raw/DeCS2020.obo\", tokenize_definition = False, tokenize_name = False)\n",
    "decs = {}\n",
    "for key,val in descriptions.items():\n",
    "    if key.startswith(\"D\"):\n",
    "        code = key\n",
    "        document = val[\"name\"]\n",
    "        if \"def\" in val:\n",
    "            document = document + \" \" + val[\"def\"]\n",
    "        decs[code] = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fasttext : si es que no está instalado fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Matias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7e77979d12fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spanish'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim # module for computing word embeddings\n",
    "import numpy as np # linear algebra module\n",
    "import sklearn.feature_extraction # package to perform tf-idf vertorization\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import fasttext.util\n",
    "import time\n",
    "stop_words = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(decs):\n",
    "    tokenized_decs = {}\n",
    "    for i in decs.keys():\n",
    "        tokenized_decs[i] = tokenized_decs[i] = mesinesp2.tokenizer.tokenizer(decs[i], split_sentences=False, is_df = False, normalize = True)\n",
    "        tokenized_decs[i] = [word for word in tokenized_decs[i] if word not in stop_words]\n",
    "    sentences = []\n",
    "    for sentence in tokenized_decs.values():\n",
    "        sentences.append(' '.join(sentence))\n",
    "    tfidfvectorizer = sklearn.feature_extraction.text.TfidfVectorizer()           # instance of the tf-idf vectorizer\n",
    "    tfidfvectorizer.fit(sentences)                                                # fitting the vectorizer and transforming the properties\n",
    "    idf = {key:val for key, val in zip(tfidfvectorizer.get_feature_names(), tfidfvectorizer.idf_)}\n",
    "    #with open(filepath, 'w+', encoding='utf-8') as json_file:\n",
    "    #    json.dump(idf, json_file, indent=2, ensure_ascii=False)\n",
    "    return idf, tokenized_decs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(text, model, idf):\n",
    "    \"\"\" Receives a sentence string along with a word embedding model and \n",
    "    returns the vector representation of the sentence\"\"\"\n",
    "    tokens = text.split() # splits the text by space and returns a list of words\n",
    "    vec = np.zeros(300) # creates an empty vector of 300 dimensions\n",
    "    for word in tokens: # iterates over the sentence\n",
    "        if (word in model) & (word in idf): # checks if the word is both in the word embedding and the tf-idf model\n",
    "            vec += model[word]*idf[word] # adds every word embedding to the vector\n",
    "    if np.linalg.norm(vec) > 0:\n",
    "        return vec / np.linalg.norm(vec) # divides the vector by their normal\n",
    "    else:\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_embeddings(sentences, model, idf, tokenized_decs):\n",
    "  vectorized_sent = [to_vector(text, model, idf) for text in sentences]\n",
    "  embedding_dict = {list(tokenized_decs.keys())[i]: vectorized_sent[i].tolist() for i in range(len(list(tokenized_decs.keys())))}   \n",
    "  return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextualized_embeddings(decs, embeddings):\n",
    "  contextualized_embeddings = {}\n",
    "  cnt = 0\n",
    "  for k, v in decs.items():\n",
    "    cnt += 1\n",
    "    if cnt%100==0: print(f'{cnt} codes transformed..')\n",
    "    s = Sentence(v)\n",
    "    embeddings.embed(s)\n",
    "    contextualized_embeddings[k] = s.embedding.detach().numpy() \n",
    "  return contextualized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf, tokenized_decs = get_idf(decs)\n",
    "\n",
    "# SBW\n",
    "start = time.time()\n",
    "sbw = KeyedVectors.load_word2vec_format(r'C:\\Users\\carol\\Desktop\\Practica 2\\SBW-vectors-300-min5.txt', limit = 100000)\n",
    "sbw_embeddings = get_pretrained_embeddings(sentences, sbw, idf, tokenized_decs)\n",
    "with open('decs_sbw.json', 'w') as fp:\n",
    "  json.dump(sbw_embeddings, fp) \n",
    "print(f'{time.time()-start} seconds to get sbw embeddings.')\n",
    "\n",
    "\n",
    "# Mix\n",
    "start = time.time()\n",
    "mix = fasttext.load_model(r'C:\\Users\\carol\\Desktop\\Practica 2\\mix_fasttext.bin')\n",
    "mix_embeddings = get_pretrained_embeddings(sentences, sbw, idf, tokenized_decs)\n",
    "with open('decs_sbw.json', 'w') as fp:\n",
    "  json.dump(mix_embeddings, fp) \n",
    "print(f'{time.time()-start} seconds to get mix embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextualized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install flair : si es que no está instalado flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import DocumentPoolEmbeddings, StackedEmbeddings, FlairEmbeddings, TransformerDocumentEmbeddings\n",
    "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert embeddings\n",
    "start = time.time()\n",
    "bert = TransformerDocumentEmbeddings(\"dccuchile/bert-base-spanish-wwm-uncased\") # Reference: https://github.com/UKPLab/sentence-transformers\n",
    "bert_embeddings = get_embeddings(decs, bert) # 768 Dimensiones\n",
    "with open('bert.json', 'w') as fp:\n",
    "    json.dump(bert_embeddings, fp)\n",
    "print(f'{time.time()-start} seconds to get bert embeddings.')\n",
    "\n",
    "# Flair emebedings\n",
    "start = time.time()\n",
    "stacked_embeddings = StackedEmbeddings(embeddings = [FlairEmbeddings('es-forward'), FlairEmbeddings('es-backward')])\n",
    "flair = DocumentPoolEmbeddings([stacked_embeddings])\n",
    "flair_embeddings = get_embeddings(decs, flair) # 4096 Dimensiones\n",
    "with open('flair.json', 'w') as fp:\n",
    "    json.dump(flair_embeddings, fp)\n",
    "print(f'{time.time()-start} seconds to get flair embeddings.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
